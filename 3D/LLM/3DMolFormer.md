### 对偶问题

* **分子对接 (Docking) - 预测问题**
    *   **输入 (已知)**：
        1.  **蛋白质口袋 (P)**：固定的3D结构
        2.  **配体化学图 (L_graph)**：固定的2D结构，即原子如何连接（通常由SMILES表示）
    *   **输出 (未知)**：
        *   **配体3D构象 (L_conf)**：配体在口袋中的最佳空间姿态和自身构象。
    *   **本质**: $P(\text{L\_conf} | \text{P}, \text{L\_graph})$，这是一个**条件概率预测**问题。可以看作是**“已知图，求几何”**
* **药物设计 (Drug Design) - 生成问题**
    *   **输入 (已知)**：
        1.  **蛋白质口袋 (P)**：固定的3D结构
    *   **输出 (未知)**：
        1.  **配体化学图 (L_graph)**：一个全新的、化学上合理的分子结构
        2.  **配体3D构象 (L_conf)**：这个新分子在口袋中的结合姿态
    *   **本质**: $P(\text{L\_graph}, \text{L\_conf} | \text{P})$，这是一个**条件概率生成**问题。可以看作是 **“给定约束，求图与几何”**
- **对偶关系分析**：
	- 对接任务中，配体的 **“身份”（化学图）是固定的**，我们求解它的 **“行为”（3D构象）**
	- 设计任务中，配体的 **“身份”是未知的**，我们需要创造出一个新的身份，并同时确定它的行为，使其能够在给定的环境中表现出色（与口袋良好结合）
- **一个绝佳的比喻：**
	*   **对接**就像是：给你一把**特定的钥匙**（配体图），让你找到它能以何种**姿态**（构象）最顺畅地插入一把**特定的锁**（蛋白口袋）
	*   **设计**就像是：只给你一把**锁**，让你从无到有地**打造**一把全新的钥匙，这把新钥匙不仅要**结构合理**，还要能完美地**匹配**这把锁

### 论文讲解

流程：蛋白质口袋原子 + 蛋白质口袋坐标 → 配体 SMILES → 配体坐标
1. 3D 坐标下口袋和配体的序列格式
2. 模型架构
	- 核心：将 token 序列中的标记嵌入（embedding）与对应数值序列中的数值**相乘**，并将这个乘积作为位置嵌入（positional embedding）的输入
		- **输入处理：**
		    1. 将 token 序列 $s_{tok}$ 通过一个标准的 Embedding 层，得到 token 嵌入向量
		    2. 将 token 嵌入向量与数值序列 $s_{num}$ 中对应位置的**浮点数**进行**逐元素相乘**
		    3. 这个乘积作为最终的输入，送入 Transformer 的位置编码层和后续的 Transformer Block
		- 为什么用乘法？
			- “调制”在信号处理中是一个核心概念，意为用一个信号去改变另一个信号的某个特性。在这里，**连续的数值 $s_{num}$ 就像一个调制信号，去改变离散的 token 嵌入向量 `embedding(s_tok)` 的“强度”或“方向”**
			- 乘法操作 `embedding * scalar` 的几何意义
				* **token 嵌入向量 (`embedding`)**：这是一个高维向量。我们可以把它想象成高维空间中的一个**方向向量**。这个向量本身编码了 token 的**语义**
					* `embedding('[x]')` 这个向量代表了“我是一个 $x$ 坐标”这个概念
					* `embedding('C')` 代表了“我是一个碳原子”这个概念
				* **数值标量 (`scalar`)**: 这是 $s_{num}$ 中的一个浮点数，比如归一化后的 $x$ 坐标值 `0.53`，或者是 $padding$ 值 `1.0`
			- 当它们相乘时：`new_embedding = embedding * scalar` 这个操作会**缩放（scale）** `embedding` 向量，例如：
				* **当 `scalar = 1.0` (Padding):
					`new_embedding = embedding * 1.0 = embedding`。向量**保持不变。这意味着，对于那些没有连续数值信息的 token（如 `[PS]`, `(`, `)`），**模型只处理它们的纯粹语义信息**，这完全符合逻辑（特殊符号的数值会设置为 `1.0`的原因）
				* **当 `scalar = 0.53` (一个坐标值):**
				    `new_embedding = embedding([x]) * 0.53` 。`embedding([x])` 这个代表 $x$ 坐标概念的向量，其**长度被缩短**了。如果 `scalar = -0.8`，向量的长度会被缩放，并且**方向会反转**
			- 通过这种方式，**连续的数值信息被编码到了高维语义向量的“模长”和“方向”上**。模型后续的层（自注意力和前馈网络）都是对向量进行操作的，它们自然能**感知到这种变化**，从而学习到相关的特征
		* **输出预测**：
		    * Transformer 的最后一层输出一个特征向量序列。这个序列被送入**两个不同**的预测头：
			    - **Token Head:** 一个标准的线性层+Softmax，用于预测下一个 token 的概率分布
			    - **Number Head:** 一个简单的线性层，输出一个**单一的浮点数**，用于预测下一个位置的数值
			- 在进行推理时也具有两种模式：
				- **Token Mode**：
					- **任务**： 从头设计一个新药分子，即生成配体的 SMILES 序列
					* **场景**：对应于 **药物设计 (Drug Design)** 任务的前半部分
					* **输入**：**只有蛋白质口袋的信息**
					* **模型的工作状态**：
					    1.  **主要目标**：预测下一个最合理的 **token** (SMILES 字符，如 'C', '(', '=', 'N' 等)
					    2.  **数值输出**：模型在每一步**也必须**输出一个数值 (通过 Number Head)。但是，当我们在生成 SMILES 字符时（比如生成一个'C'），这个位置此时**根本没有对应的 3D 坐标**。所以，Number Head的输出是**无意义的、需要被忽略的**
					    3.  **如何处理**: 这个无意义的数值输出被 **填充为1.0**
				- **Numerical Mode**：
					- **任务**：为已经确定化学结构（SMILES）的配体，生成其三维坐标。
					* **场景**：
					    1.  对应于**分子对接 (Docking)** 任务的 **全部过程** (因为对接任务的 SMILES 是已知的)
					    2.  对应于**药物设计 (Drug Design)** 任务的 **后半部分** (在 SMILES 生成之后)
					* **输入**: 蛋白质口袋信息 + **已经确定的配体 SMILES**
					* **模型的工作状态**：
					    1.  **主要目标**: 预测下一个最合理的**坐标数值** (坐标值 $x, y, z$)
					    2.  **SMILES 已经确定**： 这意味着 **token 序列的后半部分也已经完全确定了**。我们知道配体有多少个原子，因此也知道需要预测多少个 $(x, y, z)$ 坐标三元组。整个坐标部分的 token 序列就是 `[LCS], [x], [y], [z], [x], [y], [z], ..., [LCE]`。这个序列是**固定的，无需预测**
					    3.  **token 输出**：同样，Token Head 此刻也必须工作，但它的输出是**无意义的**，因为已经知道下一个 token 是什么了（即 token sequence 中的占位符 `[x]`, `[y]`, `[z]` ）
					    4.  **如何处理**：token 的输出被**忽略**，直接**填入期望的 token**。对于与 ['LCS'，'LCE'] 对应的token ，其数值设置为 1.0
3. 自监督预训练
	- 数据集：
		- Uni-Mol 的两个数据集 + CrossDocked2020
		- 大约 3.2M 个蛋白质口袋，大约 209M 个小分子构象，大约 167K 个口袋配体复合物
		
	$$
	\mathcal{L}(\hat{s}, s) = \text{CE}(\hat{s}_{\text{tok}}, s_{\text{tok}}) + \alpha \cdot \text{MSE}(\hat{s}_{\text{num}}^{\text{coord}}, s_{\text{num}}^{\text{coord}}) \tag{2}
	$$
	- **变量定义:**
	    *   $s$： 真实的训练数据序列对 ($s_{tok}$, $s_{num}$)
	    *   $\hat{s}$:：模型预测的序列对 ($\hat{s}_{tok}$, $\hat{s}_{num}$)
	    *   $CE(...)$： 交叉熵损失（Cross-Entropy Loss）。计算预测的 token 概率分布与真实 token 之间的差异
	    *   $MSE(...)$： 均方误差损失（Mean Squared Error Loss）。计算预测的坐标值与真实坐标值之间的平方差。注意，这里只对有意义的坐标部分 $s_{\text{num}}^{\text{coord}}$ 计算MSE，忽略 $padding$ 部分
	    *   $\alpha$：一个标量超参数，用于平衡两种损失的权重（默认设置为 1.0，但是其具体的值不会显著影响 CE 和 MSE 之间的平衡）
	- **直观解释:**
	    *   这是一个典型的**多任务学习**损失。模型被要求同时做好两件事：
	        1.  **学习化学语法**：通过最小化 $CE$ 损失，模型学会像语言模型一样，预测下一个合理的原子或 SMILES 字符
	        2.  **学习物理几何**：通过最小化 $MSE$ 损失，模型学会在给定上下文的情况下，预测原子的精确 3D 位置
	    *   $\alpha$ 的角色是告诉模型，这两项任务哪个更重要（或同等重要）
    - 使用自监督：从**数据本身**自动生成标签，目的是在大量数据中学会分子**通用先验知识**
4. 微调
	- **用于蛋白质-配体结合构象预测（docking）的监督微调**
		- 数据集
			- **CrossDock2020** 由对接软件 Smina 生成的（**虚拟筛选**）包含约167K的口袋-配体复合物，但其对接性能**不会超过 Smina 本身**
			- **PDBBindv2020 数据集**包含约 1.7 万个**实验**测定的口袋-配体复合物，具有更高的真实性
		- 使用 **任务特定的损失函数**——它仅对配体的 **3D 坐标**计算均方误差（MSE）损失（这是因为，在对接任务的推理过程中，模型完全工作在 **数值模式（numerical mode）** 下）
			$$
             L_{\text{docking}}(\hat{s}^{\text{lig\_coord}}, s^{\text{lig\_coord}}) = \text{MSE}(\hat{s}_{\text{num}}^{\text{lig\_coord}}, s_{\text{num}}^{\text{lig\_coord}})
			$$
		- 为减轻监督微调过程中的**过拟合**，使用 SMILES 随机化和复合体三维坐标的随机旋转作为数据增强
	- **用于口袋结合位点感知的药物设计的强化学习微调**
		- 结构：
			- 一个具备 3DMolFormer 架构的 **RL 智能体（agent）**，并使用预训练权重进行初始化
			- 为每个蛋白质口袋设计了一个**分子性质评分函数**，作为强化学习的 **奖励函数（reward）**
			- 通过迭代优化，该智能体不断调整自身策略，以最大化其生成结果的 **期望奖励**
		- 步骤：在每一个 RL 步骤中：
			1. 智能体会采样一批 3D 配体
			2. 计算每个配体的 **正则化最大似然估计（MLE）损失**
				$$L_{\text{design}}(\hat{s}^{\text{lig}}) = (\log \pi_{\text{pre\_trained}}(\hat{s}_{\text{tok}}^{\text{lig\_smiles}}) + \sigma \cdot R(m) - \log \pi_{\text{agent}}(\hat{s}_{\text{tok}}^{\text{lig\_smiles}}))^2$$
	            - 其中，$\hat{s}^{\text{lig}}$（是由强化学习（RL）智能体生成的一个样本，$m$ 是由 $\hat{s}^{\text{lig}}$ 表示的三维分子，$R(\cdot)$ 是用于评估该分子性质的奖励函数。$\pi_{\text{pre-trained}}(s)$ 表示使用预训练的 3DMolFormer 模型生成序列 $s$ 的似然，$\pi_{\text{agent}}(s)$ 是智能体当前模型生成 $s$ 的似然，$\sigma$ 是一个超参数，用于控制奖励项的重要性。
                - **变量定义:**
				    *   $\hat{s}^{\text{lig}}$：由RL agent 生成的一个配体样本，包括 $\hat{s}^{\text{lig\_smiles}}$ 和 $\hat{s}^{\text{lig\_coord}}$
				    *   $m$：由 $\hat{s}^{\text{lig}}$ 表示的 3D 分子
				    *   $\pi_{\text{agent}}(s)$： 智能体中强化学习模型生成 $s$ 的似然
				    *   $\pi_{\text{pre\_trained}}(s)$：使用预训练的 3DMolFormer 模型（**之前的步骤**得到的模型）生成序列 $s$ 的似然，作为基准（prior）
				    *   $R(m)$：**奖励函数**，用于评估生成分子 $m$ 的优劣（如对接打分、类药性等）
				    *   $\sigma$：控制奖励重要性的超参数
				- 注
					- 配体 SMILES 采样使用的是 RL 的权重，该权重会在微调过程中**持续更新**
					- 配体的 3D 坐标则使用的是对接任务微调后的模型权重，该部分在此过程中**保持不变**
			3. 利用该损失对智能体进行更新

### 一些问题

1. 乘法是怎么做的？
	```python
	# token嵌入
	token_embeddings = self.token_embed(x)  # [batch, seq_len, d_model]
	
	# 通过逐元素乘法将几何信息融入token嵌入
	# 这是关键创新点:让GPT-2能够感知3D几何信息
	
	# x_num.unsqueeze(-1): [batch, seq_len] -> [batch, seq_len, 1]
	# 广播乘法实现特征调制
	x = token_embeddings * x_num.unsqueeze(-1)
	
	# 位置编码
	# weight[: x.shape[1]]: 截取到当前序列长度，支持变长序列
	# unsqueeze(0): 添加批次维度进行广播
	x = x + self.position_embed.weight[: x.shape[1]].unsqueeze(0)
	```
2. 蛋白质口袋和配体的具体表示
	- 蛋白质口袋：原子序列按照 PDB 文件中的顺序排列，其中每个氨基酸都以 [‘N’, ‘CA’, ‘C’, ‘O’] 开头，后面是其侧链原子
	- 配体：Atom-Level Tokenization
		- 使用 **正则表达式 (Regular Expressions)** 来智能地切分 SMILES 字符串。它会将**方括号内的多字符原子（如 [NH3+], [O-]）** 或 **两位数的环闭合标记（如 %10, %11）** 作为一个**不可分割的整体**
		- **Token对应的是中心原子，而 3D 坐标也只表示这个中心原子的位置**，即 **表示重原子图 (Heavy-Atom Graph)**，编码**不包括氢原子**
			- 例如：
				- **SMILES**: [NH3+]C(C)C (异丙胺阳离子)
				- **原子级Token化结果**: ['[NH3+]', 'C', '(', 'C', ')', 'C']
3. 如何解释 分子2D表示是离散的，3D坐标是连续的？
	 - **离散信息 (Discrete Information) - 2D分子图**
		- **论文中的描述**：
		>"A 2D molecular graph describes the **2D topological structure** of a molecule, where the **nodes/atoms** are connected by **edges/bonds** respectively."
		
		2D图关心的是 **拓扑结构 (Topological Structure)**，即“连接关系”
		
		- **什么是离散的？**
			1.  **原子类型 (Atom Types / Nodes)**:
			    *   分子中的原子，是构成图的**节点 (Nodes)**
			    *   每个原子的类型（如碳C, 氮N, 氧O）来自于元素周期表中的一个**有限、可数的集合**。你不能有一个“半个碳”原子。这是一个**分类问题**，因此是**离散的**
			    *   Uni-Mol在模型中通过一个`nn.Embedding`层来处理原子类型，这正是处理离散分类数据的标准方法
			2.  **化学键类型 (Bond Types / Edges)**:
			    *   连接原子的化学键，是构成图的**边 (Edges)**
			    *   化学键的类型（单键、双键、三键、芳香键）同样来自于一个**非常小的、有限的集合**。这也是一个**分类问题**，因此是**离散的**
			    *   Uni-Mol 同样通过一个`nn.Embedding`层来处理化学键类型
			3.  **拓扑连接 (Topological Connections)**:
			    *   哪个原子和哪个原子相连，这个“连接与否”的信息是**二元的（0或1）**，是典型的**离散信息**
			    *   在图论中，这由**邻接矩阵**表示
			    *   Uni-Mol更进一步，它还考虑了**2D图中的最短路径 (Shortest Path in 2D Graph)**。两个原子在图上需要走几步才能到达，这个“步数”是一个**整数**，也是**离散的**
	-  **2. 连续信息 (Continuous Information) - 3D分子构象**
		- **论文中的描述**：
		>"A 3D molecular conformation is represented by a set of **coordinates** for all the atoms of a molecule."
		>"3D conformations focus on **spatial arranges of atoms**"
		
		这段话的核心是**空间排布 (Spatial Arranges)**，这完全是由**坐标 (Coordinates)决定的**
		
		- **什么是连续的？**
			1.  **原子坐标 (Atom Coordinates)**：
				*   每个原子在三维空间中的位置由一个 $(x, y, z)$ 坐标三元组来定义。
				*   这些 $(x, y, z)$ 的值都是**浮点数 (Floating-point numbers)**，可以在一个范围内取**任意实数值**。比如，一个原子的x坐标可以是 $1.5 Å$，也可以是 $1.5000001 Å$ 。在任意两个不同的坐标值之间，理论上都存在无限多个可能的坐标值。这就是**连续**的本质。
			2.  **原子间距离 (Interatomic Distances)**：。
				*   因为坐标是连续的，所以计算出的**距离也是一个连续的浮点数值**
			3.  **几何量 (Implicit Geometric Quantities)**：
				*   虽然Uni-Mol没有直接使用键角和二面角，但3D坐标本身就蕴含了这些信息。键角和二面角同样是**连续的角度值**