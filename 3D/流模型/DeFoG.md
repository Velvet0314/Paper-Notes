1. 基础知识
	- 离散流匹配（Discrete Flow Matching, DFM）
		$$p_{t|1}(z_t|z_1) = t \delta(z_t, z_1) + (1-t) p_0(z_t) \tag{1}$$

		* 目标：定义一个从**确定性数据点 $z_1$** 平滑地、线性地过渡到**先验噪声分布 $p_0$** 的概率路径。这个路径是所有后续推导的基石
		* **变量定义**：
		    * $z_1$：一 个离散的数据点（例如，一个节点的类别，$\text{one-hot}$ 编码）
		    * $z_t$：在时间 $t$ 的状态变量
		    * $p_{t|1}(z_t|z_1)$：给定初始数据点 $z_1$，在时间 $t$ 状态为 $z_t$ 的条件概率
		    * $t$：时间变量。$t=1$ 对应干净数据，$t=0$ 对应纯噪声
		    * $\delta(z_t, z_1)$：克罗内克（Kronecker）delta函数。当 $z_t = z_1$ 时为 1，否则为 0。在向量形式下，这代表一个在 $z_1$ 位置为 1 的 $\text{one-hot}$ 向量
		    * $p_0(z_t)$：一个预定义的先验噪声分布，通常是均匀分布 $p_0 = [1/Z, ..., 1/Z]$，其中 $Z$ 是状态总数
		* **解释:** 这个公式描述了一个非常直观的“混合”过程。
		    * 当 $t=1$ 时， $p_{1|1}(z_t|z_1) = \delta(z_t, z_1)$，概率质量完全集中在原始数据 $z_1$ 上，没有噪声
		    * 当 $t=0$ 时， $p_{0|1}(z_t|z_1) = p_0(z_t)$，数据完全被噪声替代，其分布为 $p_0$
		    * 当 $t \in (0, 1)$ 时，状态 $z_t$ 的概率是 $z_1$ 的 $\text{one-hot}$ 表示和噪声分布 $p_0$ 的**线性加权平均**。$t$ 越大，数据点“看起来”越像原始的 $z_1$
	- 连续马尔可夫链（CTMC）
		$$p_{t+dt|t}(z_{t+dt}|z_t) = \delta(z_t, z_{t+dt}) + R_t(z_t, z_{t+dt})dt \tag{2}$$
		* 目标：定义在一个**无穷小的时间步长 $dt$** 内，从当前状态 $z_t$ 转移到下一个状态 $z_{t+dt}$ 的概率。这是连续时间马尔可夫链 (CTMC) 动力学的基本方程，也是采样算法的理论基石
	    * **变量定义**：
	        * $p_{t+dt|t}(z_{t+dt}|z_t)$：从时间 $t$ 的状态 $z_t$ 到时间 $t+dt$ 的状态 $z_{t+dt}$ 的转移概率
	        * $\delta(z_t, z_{t+dt})$：克罗内克 delta 函数
	        * $R_t(z_t, z_{t+dt})$：速率矩阵 $R_t$ 中的一个元素，表示从状态 $z_t$ 到 $z_{t+dt}$ 的**瞬时转移速率**
	        * $dt$：一个无穷小的时间增量
	    * **解释**：这个公式可以分两种情况来理解，它完美地描述了在一个极短的时间内会发生什么：
	        1.  **情况一：状态发生改变 ($z_t \neq z_{t+dt}$)**
	            此时 $\delta(z_t, z_{t+dt}) = 0$，公式变为 $p_{t+dt|t}(z_{t+dt}|z_t) = R_t(z_t, z_{t+dt})dt$
	            - 在一个微小的时间窗口 $dt$ 内，系统从状态 A 跳到另一个不同状态 B 的概率，正比于从 A 到 B 的转移速率 $R_t(A, B)$ 和这个时间窗口的长度 $dt$。速率越高，时间越长，跳过去的概率就越大
	        2.  **情况二：状态保持不变 ($z_t = z_{t+dt}$)**
	            此时 $\delta(z_t, z_{t+dt}) = 1$，公式变为 $p_{t+dt|t}(z_t|z_t) = 1 + R_t(z_t, z_t)dt$
	            - 速率矩阵的对角线元素 $R_t(z_t, z_t)$ 按定义是负数，它等于所有离开该状态的速率之和的负值，即 $R_t(z_t, z_t) = -\sum_{z' \neq z_t} R_t(z_t, z')$。所以，这个表达式实际上是 $1 - (\sum_{z' \neq z_t} R_t(z_t, z')dt)$
	            - 在微小时间窗口 $dt$ 内，系统**留在**状态 A 的概率，等于1减去所有从 A **跳出去**的概率之和
	    * **推导过程**：这个公式是 **Kolmogorov 向前方程** ($\partial_t p_t = R_t^T p_t$) 的一阶**欧拉离散化 (Euler discretization)** 的结果。Kolmogorov 方程描述了概率分布 $p_t$ 随时间的连续演化。当我们要用计算机模拟这个过程时，我们无法处理连续的 $t$，必须把它切分成离散的步长 $\Delta t$
	- 去噪速率矩阵
		$$R_t^*(z_t, z_{t+dt}|z_1) = \frac{\text{ReLU}[\partial_t p_{t|1}(z_{t+dt}|z_1) - \partial_t p_{t|1}(z_t|z_1)]}{Z_t^{ \gt 0} p_{t|1}(z_t|z_1)} \tag{3}$$
		* **目标**：基于已知的加噪路径 $p_{t|1}$，构建一个连续时间马尔可夫链（CTMC）的**速率矩阵 $R_t^*$**，该矩阵能够驱动一个去噪过程，使得在任何时刻 $t$ 的状态分布都恰好是 $p_{t|1}$。这是Flow Matching思想的精髓——**用已知的流（加噪流）来匹配和定义一个未知的流（去噪流）**
		* **变量定义**：
		    * $R_t^*(z_t, z_{t+dt}|z_1)$：给定干净数据 $z_1$ 的条件下，在时间 $t$ 从状态 $z_t$ **瞬间转移**到状态 $z_{t+dt}$ 的速率。**注意，这只在 $z_t \neq z_{t+dt}$ 时定义**
		    * $\partial_t p_{t|1}(z_t|z_1)$：条件概率 $p_{t|1}$ 对时间 $t$ 的偏导数。它描述了在状态 $z_t$ 的概率质量随时间变化的“速度”
		    * $\text{ReLU}(x) = \max(0, x)$：修正线性单元。确保转移速率非负
		    * $Z_t^{\gt 0}$：在时刻 $t$ 概率不为零的状态数量
		* **解释**：这个公式的分子是核心。$\partial_t p_{t|1}$ 代表了概率流动的方向和速率
		    * 从公式(1)求导可得 $\partial_t p_{t|1}(z_t|z_1) = \delta(z_t, z_1) - p_0(z_t)$。这是一个常数，不随 $t$ 变化
		    * 分析分子 $\text{ReLU}[\partial_t p_{t|1}(z_{t+dt}|z_1) - \partial_t p_{t|1}(z_t|z_1)]$：
		        * **什么情况下会发生状态转移？** 只有当目标状态 $z_{t+dt}$ 的概率“增长速度”比当前状态 $z_t$ 的“增长速度”更快时，转移速率才大于零
		- 问题：ReLU 保证元素是非负的，为什么说速率矩阵的对角线元素 $R_t(z_t, z_t)$ 按定义是负数，它等于所有离开该状态的速率之和的负值？
			- 通过公式(3)计算出了所有**非对角线**的元素 $R_t(z_t, z_{t+dt})$ (for $z_{t+dt} \neq z_t$)，我们就可以根据“行和为零”的约束来确定对角线元素：$$R_t(z_t, z_t) + \sum_{z_{t+dt} \neq z_t} R_t(z_t, z_{t+dt}) = 0$$移项可得：$$R_t(z_t, z_t) = - \sum_{z_{t+dt} \neq z_t} R_t(z_t, z_{t+dt})$$
2. DeFoG Framework（图上的离散流匹配）
	1. 学习图上的离散流
		- 定义一个图 $G$ 为节点和边的集合 $G = (x^{1:n:N}, e^{1:i<j:N})$，$x^{1:n:N} = (x^{(n)})_{1 \leq n \leq N}, e^{1:i<j:N} = (e^{(ij)})_{1 \leq i \lt j \leq N}$，其中 $x^{(n)} \in \mathcal{X}$ 是节点 $n$ 的离散特征， $e^{(ij)} \in \mathcal{E}$ 是节点 $i$ 和 $j$ 之间边的离散特征
		- 加噪过程（Noising）
			- 将公式(1)的线性插值思想**独立地**应用到图的每一个节点和每一条边上
				$$p_{t|1}(G_t|G_1) = \prod_n p_{t|1}^{(n)}(x_t^{(n)}|x_1^{(n)}) \prod_{i<j} p_{t|1}^{(ij)}(e_t^{(ij)}|e_1^{(ij)})$$

			* **解释**：公式表明，一个完整的噪声图 $G_t$ 的生成，可以看作是所有节点和边**并行且独立地**进行各自的加噪过程。每个 $p_{t|1}^{(n)}$ 和 $p_{t|1}^{(ij)}$ 都遵循之前公式(1)定义的线性插值形式。这种分解处理极大地简化了问题，但它也带来了一个核心假设。
			* **关键假设**：
				1. 节点和边的加噪过程是**条件独立**的。虽然这在真实图结构中通常不成立，但在生成模型中，这是一个常见且有效的简化，模型的神经网络部分后续会负责学习和重建这些依赖关系。
				2. 点和边共享的初始分布，分别记为 $p_{0}^\mathcal{X}$ 和 $p_{0}^\mathcal{E}$，在正文中为简单起见假设暂时是相同的
		- 采样过程（Sampling）
			- 从一个完全随机的图 $G_0$ 开始（从预定义的初始分布 $p_0(G_0) = \prod_{n} p_0^{\mathcal{X}}(x_n^{(0)}) \prod_{i<j} p_{0}^{\mathcal{E}}(e_{ij}^{(0)})$ 中采样一个纯噪声图 $G_0$），通过一系列离散的时间步，逐步去噪，最终在 $t=1$ 时得到一个干净的图 $G_1$。每一步的“移动”都由一个速率矩阵 $R_t$ 驱动，速率矩阵由神经网络预测
				$$p_{t+\Delta t|t}(G_{t+\Delta t}|G_t) = \prod_n \tilde{p}_{t+\Delta t|t}^{(n)}(x_{t+\Delta t}^{(n)}|G_t) \prod_{i<j} \tilde{p}_{t+\Delta t|t}^{(ij)}(e_{t+\Delta t}^{(ij)}|G_t) \tag{4}$$
			* **变量定义**：
			    *   $p_{t+\Delta t|t}(G_{t+\Delta t}|G_t)$：从 $t$ 时刻的图 $G_t$ 转移到 $t+\Delta t$ 时刻的图 $G_{t+\Delta t}$ 的概率
			    *   $\Delta t$：一个有限大小的时间步长
			* **注**：
				1. 每个节点和边采用**独立**的欧拉步长
				2. 在时刻 $t$，为了得到 $G_{t+\Delta t}$，我们**独立地为每个节点和每条边**计算它们的转移概率，然后从这些概率中采样出它们的新状态。整个图的更新是并行完成的
			 $$R_t^{(n)}(x_t^{(n)}, x_{t+dt}^{(n)}) = \mathbb{E}_{p_{1|t}^{(n)}(x_1^{(n)}|G_t)}[R_t^{*(n)}(x_t^{(n)}, x_{t+ \Delta t}^{(n)}|x_1^{(n)})] \tag{5}$$
			* **变量定义**：
				* $R_t^{(n)}$：在时刻 $t$ 用于节点 $n$ 采样的**无条件**速率矩阵
				* $p_{1|t}^{(n)}(x_1^{(n)}|G_t)$：神经网络 $f_\theta$ 输入噪声图 $G_t$ 后，对节点 $n$ 的**干净状态** $x_1^{(n)}$ 做出的**预测概率分布**
				* $R_t^{*(n)}(...|x_1^{(n)})$：由公式(3)定义的、**以真实的干净状态 $x_1^{(n)}$ 为条件**的理想速率矩阵
			* **解释**：通过公式(3)，只要知道了最终目标 $x_1$，就能算出完美的速率矩阵 $R_t^*$。但在采样时，我们并不知道 $x_1$。我们所拥有的，是神经网络对 $x_1$ 可能是什么样子的一个**概率性猜测** $p_{1|t}^{(n)}$。公式(5)做的就是，将理想的速率矩阵 $R_t^*$ 在这个神经网络给出的“信念分布” $p_{1|t}^{(n)}$ 上进行**期望（加权平均）**。换句话说，我们基于模型的所有可能预测，计算出一个平均的、最合理的演化方向。
			* **与模型的联系**：这清晰地展示了**训练与采样的解耦**。
				* **训练时 (Algorithm 1)**：神经网络 $f_\theta$ 的唯一任务就是学习输出一个准确的概率分布 $p_{1|t}^{(n)}(\cdot|G_t)$
				* **采样时 (Algorithm 2)**：我们将 $f_\theta$ 的输出 $p_{1|t}^{(n)}$ **代入**公式(5)，计算出速率矩阵 $R_t^{(n)}$，再利用这个速率矩阵和公式(4)来执行一步采样
		- 训练过程（Training）
			- 损失函数
				$$\mathcal{L}_{\text{DeFoG}} = \mathbb{E}_{t \sim \mathcal{T}, G_1 \sim p_1(G_1), G_t \sim p_{t|1}(G_t|G_1)} \left[ \text{CEx}(G_1, \hat{p}_{1|t}(\cdot|G_t)) \right] $$
				其中$$\text{CEx}(G_1, \hat{p}_{1|t}(\cdot|G_t)) = -\sum_n \log(\hat{p}_{1|t}^{\theta,(n)}(x_1^{(n)}|G_t)) - \lambda \sum_{i<j} \log(\hat{p}_{1|t}^{\theta,(ij)}(e_1^{(ij)}|G_t))$$
			* **目标**：训练一个神经网络 $f_\theta$（参数为 $\theta$），使其能够在给定任意时刻的噪声图 $G_t$ 后，准确地预测出原始干净图 $G_1$ 的节点和边类别
			* **解释**：
			    * 这是一个标准的监督学习范式
			    * $\mathbb{E}[\cdot]$：通过从训练集采样干净图 $G_1$，随机采样时间 $t$，并根据公式(1)生成噪声图 $G_t$ 来近似这个期望
			    * $\text{CEx}$：交叉熵损失函数。它衡量了模型预测的干净图概率分布 $\hat{p}_{1|t}$ 与真实干净图 $G_1$（看作 $\text{one-hot}$ 真值）之间的差距
			    * $\lambda$：一个超参数，用于平衡节点属性和边属性损失的重要性
		- 训练与采样的解耦
			- 解耦在哪里？
				- **训练的唯一职责 (Training's Job)**：学习一个函数 $f_\theta$（神经网络），输入一个任意时刻的噪声图 $G_t$，输出对原始干净图 $G_1$ 的概率预测 $\hat{p}_{1|t}(\cdot|G_t)$。**在训练期间，模型完全不知道、也不关心未来采样时会使用什么样的速率矩阵 $R_t$**
				* **采样的独立设计 (Sampling's Job)**：采样过程则是一个独立的模块。，在每一步，它调用训练好的 $f_\theta$ 得到对干净图的预测，然后**独立地**决定使用哪种速率矩阵公式来计算下一步的转移概率，并进行采样
			- 理论保证
				- **Corollary 1 (Bounded estimation error of rate matrix for graphs) 图的速率矩阵的有界估计误差**
					$$|R_t(G_t, G_{t+dt}) - \hat{R}_t^\theta(G_t, G_{t+dt})|^2 \le C_0 + C_1 \mathbb{E}_{p_1(G_1)} \left[ p_{t|1}(G_t|G_1) \sum_n -\log \hat{p}_{1|t}^{\theta,(n)}(x_1^{(n)}|G_t) \right] + C_2 \mathbb{E}_{p_1(G_1)} \left[ p_{t|1}(G_t|G_1) \sum_{i<j} -\log \hat{p}_{1|t}^{\theta,(ij)}(e_1^{(ij)}|G_t) \right]$$
					* **变量定义**：
					    * $R_t(G_t, G_{t+dt})$：**理论上完美**的、真实的速率矩阵。它是在已知真实干净图 $G_1$ 的情况下计算出来的
					    * $\hat{R}_t^\theta(G_t, G_{t+dt})$：**我们实际使用**的、近似的速率矩阵。它是基于神经网络 $f_\theta$ 的**预测** $\hat{p}_{1|t}^\theta$ 计算出来的
					    * $| \cdot |^2$：左边项表示我们实际使用的速率矩阵与完美速率矩阵之间的**均方误差**。这个值越小，说明我们的采样过程越接近理想情况
					    * $\sum_n -\log \hat{p}_{1|t}^{\theta,(n)}(\cdot)$：这正是我们训练模型时使用的**节点部分的交叉熵损失**
					    * $\sum_{i<j} -\log \hat{p}_{1|t}^{\theta,(ij)}(\cdot)$：这正是**边部分的交叉熵损失**
					    * $\mathbb{E}_{p_1(G_1)}[p_{t|1}(G_t|G_1) \cdot]$：对所有可能的干净图 $G_1$ 和其对应的噪声图 $G_t$ 求期望。这与DeFoG的训练损失 $\mathcal{L}_{\text{DeFoG}}$ 的形式是完全一致的
					* **解释**：
						- **左边**是**采样阶段的误差**（速率矩阵的准确性）
						- **右边**是**训练阶段的目标**（交叉熵损失）
					    - 不等式表明，**采样误差**被一个**与训练损失成正比**的项所**上界控制**。这意味着：
						    * 当我们通过训练去最小化交叉熵损失（右边项）时，我们必然会压缩速率矩阵估计误差（左边项）的上界，从而间接地使其减小
						    * **训练目标和采样目标**是完全对齐的 (aligned)。 **优化其中一个，必然会改进另一个**
					* **与模型的联系**：
						- Corollary 1是在固定的 $t$ 和 $G_t$ 下成立的。如果我们对所有可能的 $t$ 和 $G_t$ 求期望，那么不等式的右边就变成了DeFoG的整体训练损失 $\mathcal{L}_{\text{DeFoG}}$（当 $\lambda=1$ 时）。因此，**最小化 $\mathcal{L}_{\text{DeFoG}}$ 就等价于最小化采样误差的全局上界**
				- **Corollary 2 (Bounded deviation of the generated graph distribution) 生成分布的偏差上界**
					$$\| p_1(G_1) - p_{\text{data}} \|_{TV} \le \bar{U} \left( XN + E\frac{N(N-1)}{2} \right) + \bar{B} \left( XN + E\frac{N(N-1)}{2} \right)^2 \Delta t + O(\Delta t^2)$$
					- **变量定义**：
					    * $p_1(G_1)$：我们通过DeFoG的**实际采样算法**（Algorithm 2）在 $t=1$ 时刻生成的图所遵循的**概率分布**。这是我们关心和评估的对象
					    * $p_{\text{data}}$：**真实的、我们想要学习的目标数据分布**。在理论推导中，它等价于一个**理想的、连续时间**CTMC演化到 $t=1$ 时的分布 $\tilde{p}_1$
					    * $\| \cdot \|_{TV}$：**总变差距离 (Total Variation distance)**。这是一种衡量两个概率分布之间差异的度量。其值在0和1之间，0表示两个分布完全相同，1表示完全不同。这个值越小，说明我们生成的分布越接近真实分布
					    * $XN + E\frac{N(N-1)}{2}$：图中离散变量的总数。$N$ 是节点数，$X$ 是节点类别数；$E$ 是边类别数，$\frac{N(N-1)}{2}$ 是无向完全图的边数
					    * $\Delta t$：采样过程中使用的**最大时间步长**。
					    * $\bar{U}$：一个常数上界，其大小与 **Corollary 1** 中定义的**速率矩阵估计误差**有关
					    * $\bar{B}$：一个常数上界，与CTMC过程本身的性质（如速率矩阵对时间的导数）有关
					    * $O(\Delta t^2)$：高阶无穷小项
					* **直观解释**：这个不等式将最终的生成分布误差 $\| p_1(G_1) - p_{\text{data}} \|_{TV}$ 分解为两个主要来源，并清晰地指出了如何控制它们：
					    1.  **第一项： $\bar{U} \left( \dots \right)$ —— 模型预测误差 (Estimation Error)**
					        * 来源：这一项源于我们的神经网络 $f_\theta$ **不是完美**的。它的预测 $\hat{p}_{1|t}$ 与真实情况存在偏差，导致我们计算出的速率矩阵 $\hat{R}_t^\theta$ 是一个近似值
					        * 与Corollary 1的联系：$\bar{U}$ 这个常数本质上是由Corollary 1中的速率矩阵误差 $|R_t - \hat{R}_t^\theta|^2$ 决定的。正如Corollary 1所说，**只要我们通过训练最小化交叉熵损失，就能减小这个速率矩阵误差，从而减小 $\bar{U}$**
					        * **如何控制**：**更好的模型架构、更长的训练时间、更高质量的数据**，都可以让模型预测更准，从而减小这一项误差
					    2.  **第二项： $\bar{B} \left( \dots \right)^2 \Delta t$ —— 离散化误差 (Discretization Error)**
					        * 来源：这一项源于我们用**有限的、离散的步长 $\Delta t$** 去模拟一个**理论上连续的时间过程**。这就好比用一系列短直线去逼近一条光滑曲线，步长越大，近似得就越粗糙，误差也就越大。
					        * 如何控制：这一项与 $\Delta t$ 成正比。因此，我们可以通过**减小采样步长 $\Delta t$**（即增加总采样步数）来使其任意小。当 $\Delta t \to 0$ 时，这一项误差也趋向于0
					- **与模型的联系**：
						- DeFoG的采样算法是**收敛的**。我们有两个明确的方向可以调节来提升生成质量：
					        1.  **训练**：把模型训练得更好，减小第一项误差
					        2.  **采样**：用更精细的步长（更多的步数）进行采样，减小第二项误差
				- **总结**：
					1.  **起点: 训练损失 $\mathcal{L}_{\text{DeFoG}}$**
					    *   这是我们可以直接优化的目标
					2.  **通过 Corollary 1 连接到...**
					    *   单步采样误差 (速率矩阵误差 $|R_t - \hat{R}_t^\theta|^2$)
					    *   **结论**：最小化训练损失 $\implies$ 减小单步采样误差
					3.  **通过 Corollary 2 连接到...**
					    *   最终生成分布误差 $\| p_1 - p_{\text{data}} \|_{TV}$
					    *   **结论**：减小单步采样误差 (来自Corollary 1) + 减小采样步长 $\Delta t$ $\implies$ 减小最终生成分布的误差
	2. DeFoG 的设计空间
		1. **采样失真 (Sample Distortion)**
			* **动机**：均匀的时间步长 $\Delta t$ 可能不是最优的。例如，对于需要满足严格约束（如平面性）的图，在去噪的最后阶段（$t \to 1$）进行精细微调至关重要
			* **方法**：通过一个单调函数 $t' = f(t)$ 对时间进行重映射，使得在关键区域（如 $t$ 接近1时）的有效步长更小，而在非关键区域步长更大，从而在总步数不变的情况下实现更精细的控制
		2. **训练失真 (Train Distortion)**
			* **动机**：与采样失真相辅相成。如果我们通过实验发现某个时间段对生成至关重要，我们也可以在训练时“扭曲”时间的采样分布 $\mathcal{T}$，让模型更多地关注和学习这些关键时间段的去噪任务，从而提升其在这些区域的预测精度。
		3. **目标引导 (Target Guidance)**
			* **动机**：标准的速率矩阵 $R_t^*$ 是隐式地、平滑地引导系统走向目标。我们能否更“激进”一些，直接给系统一个朝向预测目标的额外推力？
				$$R_t^\omega(z_t, z_{t+dt}|z_1) = \omega \cdot \frac{\delta(z_{t+dt}, z_1)}{\mathcal{Z}_t^0 p_{t|1}(z_t|z_1)} \tag{6}$$
				* **变量定义**：
				    * $\omega$：引导强度，一个超参数
				    * $z_1$：模型预测的干净状态
				* **解释**：这个附加项非常直接：它只在目标状态 $z_{t+dt}$ 正是模型预测的干净状态 $z_1$ 时才非零。这意味着它在原有的速率矩阵之上，额外增加了**一个直接跳向最终预测目标的速率**。当 $\omega > 0$ 时，模型会被更强地“拉”向它认为最可信的干净状态。这在采样步数较少时尤其有效，可以加速收敛
				* **潜在问题**：附录B.2的理论分析指出，这种引导方式会引入 $O(\omega)$ 的误差，破坏了流匹配所依赖的 Kolmogorov 方程。因此，$\omega$ 不能设置得过大，否则会导致生成分布偏离真实分布，牺牲多样性来换取“看似正确”的样本
		4. **随机性 (Stochasticity)**
			* **动机**：仅由 $R_t^*$ 驱动的去噪过程可能过于确定性，容易陷入局部最优。引入适度的随机性可以帮助模型探索更广阔的解空间，并可能纠正早期的预测错误
			* **方法**：在原速率矩阵基础上增加一个满足**细致平衡条件 (Detailed Balance)** 的矩阵 $R^{\text{DB}}$，即 $R_t' = R_t^* + \eta R^{\text{DB}}$。增加这样的项不会改变过程的稳态分布，但会增加状态间的跳转，从而提高轨迹的随机性，其强度由超参数 $\eta$ 控制
	3. 排列不变性保证
		- **目标**：确保DeFoG作为一个图生成模型，其行为（训练损失和采样概率）与图中节点的输入顺序无关。这是一个图神经网络模型必须满足的基本性质。论文通过**引理3 (Lemma 3)** 对此进行了形式化证明
		* **核心思想**：模型的排列不变性来源于其各个组件的协同作用：
		    1.  **等变的网络架构 (Equivariant Architecture)**：DeFoG采用的图Transformer网络本身被设计为**排列等变的 (permutation equivariant)**。这意味着如果输入图的节点顺序被打乱，输出的节点/边预测的顺序也会相应地、一致地被打乱，但预测值本身不变
		    2.  **不变的损失函数 (Invariant Loss Function)**：损失函数是所有节点和边的交叉熵损失的**总和**。求和操作是排列不变的，无论你按什么顺序加，结果都一样。因此，无论节点如何排序，计算出的总损失值是相同的
		    3.  **不变的采样概率 (Invariant Sampling Probability)**：公式(4)表明，图的转移概率是所有节点和边转移概率的**乘积**。与求和类似，乘法也是排列不变的。因此，生成一个特定图（忽略节点标签顺序）的最终概率与中间的节点排序无关
		* **结论**：由于模型的核心组件（网络、损失、采样）都正确地处理了图的对称性，DeFoG框架被保证是排列不变的，这是一个健全的图生成模型所必需的