1. 流匹配的基本知识
	- **连续数据的流匹配（Flow Matching）**
		- 流匹配是一种生成模型框架，其目标是通过学习一个向量场 $v_t^\theta(x_t)$，将样本从初始分布 $p_0$ 迁移到目标分布 $p_1$。这个向量场会诱导一个随时间变化的密度 $p_t$，其两端点分别是 $p_0$ 和 $p_1$
		- 流匹配的关键思想在于可以通过以下步骤来学习该向量场：首先从目标分布采样 $x_1 \sim p_1(x_1)$，然后从条件概率路径 $x_t \sim p_{t|1}(x_t|x_1)$ 中采样，该路径对应一个向量场 $u_t(x_t|x_1)$，最后将 $v_t^\theta(x_t)$ 回归拟合到 $u_t(x_t|x_1)$ 上
			- $p_{t|1}$​ 里的下标表示：时间为 $t$，且条件在终点分布 $x_1$ 上
			- $x_t \sim p_{t|1}(x_t|x_1)$ 表示给定一个从目标分布采到的样本 $x_1$​，我们再在路径中间的时间 $t$ 采样 $x_t$​
	- **离散流模型（Discrete Flow Models）**
		- 有研究提出基于 **连续时间马尔可夫链（CTMC）** 的序列生成框架，将流匹配方法扩展到 **离散类别数据**。这些方法与连续流匹配类似：首先定义条件概率路径 $p_{t|1}(\cdot|x_1)$，然后学习一个数据去噪器 $p_\theta^{t|1}(\cdot|x_t)$，在推理过程中利用它将 $x_t$​ 推向真实数据分布
2. 模型架构
	- 基于**SemlaFlow** 模型 —— SemlaFlow 在 **无条件三维分子生成任务** 上取得了当前最优的结果，其核心的 **Semla 架构** 是一种 **E(3)-等变架构**，通过多项创新显著提升了效率与可扩展性
	- 对 Semla 架构进行了扩展，使其能够支持**条件生成**
		- **口袋编码器（pocket encoder）** —— 其不依赖于时间步 $t$ 或配体 $l_t$​，因此在生成配体时仅需一次前向传播即可完成编码，进一步保证了方法的高效性
		- **配体解码器（ligand decoder）** —— 其中加入了**交叉注意力模块**。该模块的设计与 Semla 中提出的注意力模块类似，使用 **两层 MLP** 生成注意力分数。它会同时接收**口袋 $\mathcal{P}$ 的不变与等变嵌入** 以及 **配体 $l_t$​** 的嵌入，并可选地引入 **相互作用信息 $\mathcal{I}$**，从而实现基于蛋白质口袋及期望相互作用的结构条件约束（使用**潜在注意力操作（latent attention operation）**，以显著提升该过程的效率）
	- 对 Semla 内部的一些现有组件进行了改进，从而进一步提升了模型的性能与效率
		- 基于**门控(gating)的等变前馈网络**替换了 Semla 中的等变前馈模块(equivariant feed-forward module)。具体来说，如果原子 $i$ 的不变与等变输入特征分别记为 $h_i \in \mathbb{R}^{d_{inv}}$ 和 $x_i \in \mathbb{R}^{3 \times d_{equi}}$，那么输出为：
			$$x_i^{out} = W_\theta^2 \tilde{x}_i \quad \text{其中} \quad \tilde{x}_i = \sigma(\Phi_\theta(h_i, ||x_i||)) \odot W_\theta^1 x_i$$

			这里 $\sigma$ 表示应用于不变特征的元素级 sigmoid 函数，$\odot$ 表示逐元素乘法，$W_\theta^1 \in \mathbb{R}^{d_{equi} \times d_{equi}}$ 和 $W_\theta^2 \in \mathbb{R}^{d_{equi} \times d_{equi}}$ 都是权重矩阵，$\Phi_\theta$ 是一个两层的MLP
			- 该模块的速度显著快于 Semla 使用的等变前馈模块
		- 在每一层的自注意力模块中都传递了**键的嵌入（bond embeddings）**，而不是像 Semla 那样仅在第一层传递。这一改动提升了生成分子的有效性，同时对推理时间的影响非常小
			- 在配体原子之间进行每一次信息交换时，都不断地提醒模型它们之间的**共价键结构**。这有助于模型更好地维持分子的化学合理性，为空间上的注意力机制增加了一层基于拓扑结构的强先验
3. 训练与推理
	- 模型训练
		- **目标**：生成以给定结构为条件的新配体
		- 由于3D分子图包含**连续和分类数据类型**的混合，FLOWR联合生成连续和离散分布，使用连续的流匹配和离散六模型。配体形式电荷不通过流学习，而是简单地通过模型预测
		- 训练通过采样配体噪声 $l_0 \sim p_{noise}$、配体、口袋和相互作用拓扑 $(l_1, \mathcal{P}, \mathcal{I})$ 在时间 $t \in [0,1]$ 进行。我们使用高斯噪声用于坐标和原子和键类型的均匀分布来创建 $p_{noise}$。然后从相同的条件概率路径 $l_t \sim p_{t|1}(l_t|l_1)$ 中采样一个噪声配体，该路径定义如下：
			$$t \sim \text{Beta}(\alpha, \beta) \quad x_t \sim \mathcal{N}(tx_1 + (1-t)x_0, \sigma^2) \quad (1)$$$$a_t \sim \text{Cat}(t\delta(a_1) + (1-t)\frac{1}{|A|}) \quad b_t \sim \text{Cat}(t\delta(b_1) + (1-t)\frac{1}{|B|}) \quad (2)$$
			- 其中 $A$ 和 $B$ 分别是**原子类型和键序的可能值集合**，$\delta(.)$是应用于序列中每个项目的独热编码操作。论文中对所有 FLOWR 模型使用值 $\alpha = 2.0, \beta = 1.0$ 和 $\sigma = 0.2$ 
			- Beta 分布
				$$f(x;\alpha,\beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$$
				- 其中 $B(\alpha, \beta) = \int_0^1 t^{\alpha-1}(1-t)^{\beta-1} dt$，那么在 $\alpha = 2.0, \beta = 1.0$ 时其概率密度函数为 $f(x)=2x$，所以概率密度随着 $x$ 的增大而线性增加，这表示从这个分布中随机抽取一个数，**抽到接近 1 的数的可能性，要远大于抽到接近 0 的数**
				- 意义：从纯噪声开始的早期阶段，向量场的方向相对简单（大致指向数据中心即可），可能不需要那么密集的训练，**对于生成高质量的分子而言，学习如何在最后阶段精确地去除少量噪声，比学习如何从一团乱麻中构建出大致轮廓更为关键和困难**