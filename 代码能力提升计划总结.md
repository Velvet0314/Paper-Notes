### 📘 VGAE 数学公式整理

#### 1. 图卷积网络 (GCN)

标准图卷积层公式：
$$H^{(l+1)} = \sigma \Big( \hat{A} H^{(l)} W^{(l)} \Big)$$
其中：
- $H^{(l)}$ 表示第 $l$ 层的节点表示，初始时 $H^{(0)} = X$（输入特征）
- $W^{(l)}$ 是可学习权重矩阵
- $\sigma(\cdot)$ 是激活函数（例如 ReLU）
- $\hat{A}$ 是归一化邻接矩阵：$$\hat{A} = D^{-\frac{1}{2}} (A + I) D^{-\frac{1}{2}}$$

	- 其中 $A$ 是原始邻接矩阵，$I$ 是单位矩阵（自环），$D$ 是度矩阵，$D_{ii} = \sum_j (A+I)_{ij}$。

#### 2. VGAE 编码器 (Encoder)

VGAE 假设潜变量分布为：$$Z \sim q_\phi(Z|X,A) = \prod_{i=1}^N \mathcal{N}(z_i \mid \mu_i, \text{diag}(\sigma_i^2))$$第一层 GCN 得到隐藏表示：$$H = \text{ReLU}\big(\hat{A} X W^{(0)}\big)$$
再通过两层 GCN 分别得到均值和对数方差：$$\mu = \hat{A} H W^{(1)}_\mu$$
$$\log \sigma = \hat{A} H W^{(1)}_\sigma$$
采用重参数化技巧：$$z_i = \mu_i + \epsilon_i \odot \sigma_i, \quad \epsilon_i \sim \mathcal{N}(0, I)$$
#### 3. VGAE 解码器 (Decoder)

解码器使用内积形式：$$\hat{A}_{ij} = \sigma(z_i^\top z_j)$$其中 $\sigma(\cdot)$ 是 Sigmoid，$\hat{A}$ 是预测得到的邻接矩阵（边的存在概率）

#### 4. 损失函数 (Loss)

变分自编码器的目标函数是证据下界（ELBO）：$$\mathcal{L} = \mathbb{E}_{q_\phi(Z|X,A)}[\log p_\theta(A|Z)] - \text{KL}\big(q_\phi(Z|X,A) ,|, p(Z)\big) $$
- **重构损失**（边预测的二元交叉熵）：$$ \log p(A|Z) = \sum_{i,j} \Big[ A_{ij}\log \hat{A}_{ij} + (1-A_{ij}) \log (1-\hat{A}_{ij}) \Big] $$
- **KL 散度正则化**：$$  \text{KL} = -\frac{1}{2} \sum_{i=1}^N \sum_{k=1}^d \Big( 1 + \log \sigma_{ik}^2 - \mu_{ik}^2 - \sigma_{ik}^2 \Big) $$
### 问题

 1. `practice2` 中 `recon_loss` 很大，而且会出现 $\text{kl\ \_\ loss} = -0.000$
	 - 原因：
		 - `recon_loss` 很大
			 - 真实的图较为稀疏 `A = (torch.rand(n, n) <= 0.3).float()`，模型很容易倾向于预测所有位置都是 0（非边），因为这样能让大部分预测正确
			 - $BCE = -[y\ log(p) + (1-y)\ log(1-p)]$  
				 - 当 $y=0$（非边）很多时： 如果模型预测 $p≈0.5$（不确定），$loss = -log(0.5) ≈ 0.69$ （70个非边 × 0.69 ≈ 48.3）
				 - 然后 loss 采用累加方式计算导致累加后值很大
		- `kl_loss` 出现负数
			- 数值不稳定导致
	- 改进方法：（在 `practice3` 中进行）
		- 目前没有采样标准 VGAE 的做法
			- 标准做法：边列表表示
		- 确保数值的稳定性，添加稳定性策略
			- 标准做法：
				1. `EPS` = 1e-15
				2. `MAX_LOGSTD` = 10 ——  $σ=e^{10}≈2.2×10^4$
					- 当 $\log\sigma$ 太大或太小时：
						- $e^{\log\sigma}$ 可能会 数值爆炸（Inf） 或 趋于 0
					    - 导致梯度不稳定，loss 出现 `nan`