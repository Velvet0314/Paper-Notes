<div style="text-align: center;"> <img src="few-shot.png" width="700"> </div>

### **1. 数据层面 (Data-level): “如果数据不够，就创造或善用数据”**

这类方法的核心思想是，直接从数据本身入手来缓解稀疏性问题。

#### **1.1 生成式分子数据增强 (Generative Molecule Data Augmentation)**
*   **核心思想：** 直接凭空创造出与小样本类别相似的、新的、有效的分子数据，从而扩充样本集。
*   **如何解决FSL：** 这是最直接的思路。如果每个类别只有5个样本，模型很难学。但如果我们能用生成模型（如 VAEs, GANs, Diffusion Models）学习这5个样本的潜在分布，然后采样生成50个新的、多样但相似的分子，那么小样本问题就变成了传统监督学习问题。
*   **在分子领域的特殊性：** 与图像（可以简单地旋转、裁剪）不同，分子的增强必须保证**化学有效性 (validity)** 和**性质相关性 (property correlation)**。生成的新分子必须是化学上成立的，并且其性质应该与我们关注的目标任务（如高活性）保持一致。例如，`Meta-MGNN` 等模型会学习一个生成器，专门为小样本任务生成新的分子图。

#### **1.2 隐式分子关系构建 (Implicit Molecule Relation Construction)**
*   **核心思想：** 单个分子提供的信息有限，但分子之间的关系（如相似性、包含共同官能团）蕴含着丰富信息。通过构建一个“关系图”，让模型不仅学习分子本身，也学习分子间的上下文。
*   **如何解决FSL：** 这种方法将学习对象从“孤立的分子”转变为“关系网络中的节点”。模型可以利用“邻居”分子的信息来辅助对当前分子的判断。例如，在一个任务中，如果分子A和分子B结构相似（关系），而我们知道分子B是高活性的（标签），那么模型可以推断分子A也很可能是高活性的，即使训练集中没有直接的证据。
*   **在分子领域的实践：** `PAR` 和 `Meta-Link` 等方法会构建一个分子间的图，节点是分子，边代表它们之间的化学相似度（如Tanimoto系数）。然后，它们在这个图上传播标签信息或学习图嵌入，从而为每个分子赋予更丰富的、包含上下文信息的表示。

#### **1.3 混合方法 (Hybrid Methods)**
*   **核心思想：** 结合上述两种方法的优点。
*   **如何解决FSL：** 先通过生成模型扩充数据（如1.1），然后在包含原始数据和生成数据在内的更大集合上构建关系网络（如1.2）。这既增加了数据量，又利用了数据间的结构信息，是一种强有力的组合策略。

---

### **2. 模型层面 (Model-level): “如果数据有限，就设计更聪明的模型”**

这类方法的核心是，改进模型架构，使其天生就具备强大的泛化能力和从少量样本中提取本质特征的能力。

#### **2.1 分子内在表示学习 (Molecular Intrinsic Representation Learning)**
*   **核心思想：** 这是**迁移学习 (Transfer Learning)** 的经典应用。先在海量的、无标签或有其他标签的分子数据上进行**预训练 (Pre-training)**，让模型学会通用的“化学语言”。然后，在面对新的小样本任务时，只需对模型进行微调 (Fine-tuning)。
*   **如何解决FSL：** 预训练阶段让模型掌握了关于原子、化学键、官能团、空间结构等基础且普适的化学知识。这些知识被编码在模型的参数中。因此，当面对一个只有5个样本的新任务时，模型不需要从零开始学习，它只需要利用已经学到的强大表示能力，去学习这个新任务的“顶层决策边界”即可，这大大降低了对数据的需求。
*   **在分子领域的实践：** `SMF-GIN` 和 `FS-GNNTR` 等模型会使用如图神经网络（GNN）或Transformer等架构，在数百万甚至上亿的分子数据库（如ZINC, PubChem）上进行自监督学习（如预测被遮盖的原子、对比学习等），然后再应用于下游的小样本任务。

#### **2.2 分子上下文感知学习 (Molecular Context-aware Learning)**
*   **核心思想：** 让模型在预测时，能够**动态地**参考和比较小样本支持集（Support Set）中的所有样本。
*   **如何解决FSL：** 这类方法通常遵循**元学习 (Meta-Learning)** 中的度量学习范式。模型学习一个度量空间，在这个空间里，具有相同属性的分子距离更近。在预测一个新分子（Query）时，模型会计算它与支持集中各个类别分子的距离或关系。例如，`Prototypical Networks` 的思想是为每个类别计算一个“原型”（该类别所有样本表示的平均值），然后看新分子离哪个原型最近。
*   **在分子领域的实践：** `CAMP` 和 `MHNfs` 等模型会设计特定的注意力机制或图匹配网络，来显式地建模查询分子与支持集中每个分子之间的交互和相似性，从而做出更具鲁棒性的预测。这与1.2的区别在于，关系构建是模型推理的一部分，而不是数据预处理。

#### **2.3 基于适配器的泛化策略 (Adapter-based Generalization Strategies)**
*   **核心思想：** 这是对2.1中微调策略的一种优化，旨在实现**参数高效的微调 (Parameter-Efficient Fine-Tuning, PEFT)**。
*   **如何解决FSL：** 在微调一个巨大的预训练模型时，如果更新所有参数，依然有过拟合的风险，并且计算成本高。Adapter策略是：**冻结预训练模型的绝大部分参数**，只在模型中插入一些轻量级的、可训练的“适配器”模块。对于每个新的小样本任务，只训练这些适配器模块的参数。这样既利用了预训练模型的强大能力，又因为可训练参数极少，从而有效避免了过拟合。
*   **在分子领域的实践：** `ATGNN` 和 `Pin-Tuning` 就是将这种思想应用于分子的GNN模型，在GNN的层与层之间插入小的MLP作为适配器进行训练。

---

### **3. 学习范式 (Learning Paradigm): “如果数据和模型都定了，就改变训练方法”**

这类方法关注于优化算法和训练流程，使其能更好地适应小样本的学习目标。

#### **3.1 重构参数优化策略 (Reformulated Parameter Optimization Strategies)**
*   **核心思想：** 传统的优化目标是“在当前任务上表现好”，而元学习的优化目标是“**学习如何快速学习**”或“找到一个好的出发点，以便能快速适应新任务”。
*   **如何解决FSL：** 典型代表是**MAML (Model-Agnostic Meta-Learning)**。它通过在大量不同的模拟小样本任务上进行训练，寻找一个模型的初始参数点。这个初始点并非在任何单一任务上最优，但它具有最大的“泛化潜力”，只需在新任务的几个样本上进行一两步梯度下降，就能迅速达到很好的性能。它本质上是学会了一种“快速适应”的能力。
*   **在分子领域的实践：** `ADKF-IFT` 等模型将这类元学习优化算法与分子模型相结合，训练模型使其能够从一个好的参数初始状态出发，快速泛化到新的分子属性预测任务上。

#### **3.2 其他策略 (Other Strategies)**
*   **核心思想：** 包含一些独特的、不完全属于上述类别的学习策略。
*   **如何解决FSL：** 例如，`Meta-MolNet` 学习一个任务自适应的度量空间，即针对不同的分子属性预测任务，动态生成不同的距离度量函数，这比使用固定度量函数（如欧氏距离）更加灵活和有效。`AR-APM` 可能采用贝叶斯方法或任务关系建模等其他高级技术来处理小样本问题。
